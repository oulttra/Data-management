{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oulttra/Data-management/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijNjInPj8bSP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import xgboost\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ3bDymrJPvR"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJiTBp3485hY"
      },
      "outputs": [],
      "source": [
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "3RBebr0CK-DH",
        "outputId": "1d3079b6-8ca7-4c6e-e286-069bd31df284"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af551f83-6968-4c82-87d0-b30b9d96af71\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af551f83-6968-4c82-87d0-b30b9d96af71\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PluCubt3M_r0"
      },
      "outputs": [],
      "source": [
        "print(uploaded.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rQw37eCdeMq"
      },
      "outputs": [],
      "source": [
        "database1 = pd.read_csv(io.BytesIO(uploaded['database1 (4).csv']))\n",
        "database2 = pd.read_csv(io.BytesIO(uploaded['database2 (4).csv']))\n",
        "database3 = pd.read_csv(io.BytesIO(uploaded['database3 (4).csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edYOnyi9Kq39"
      },
      "outputs": [],
      "source": [
        "print(database1 .head())\n",
        "print(database2 .head())\n",
        "print(database3 .head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22Z_5lTQinMb"
      },
      "outputs": [],
      "source": [
        "print(database1.columns)\n",
        "print(database2.columns)\n",
        "print(database3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi_iKq8RkIuq"
      },
      "outputs": [],
      "source": [
        "if 'hhid' in database1.columns:\n",
        "    database1.rename(columns={'hhid': 'HHID'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYaNB17SOnZh"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(database1, database2, on=\"HHID\")\n",
        "# we merge database1 and database 2 using HHID columns. We named it merged_df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4w_3gguo2aX"
      },
      "outputs": [],
      "source": [
        "print(merged_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6tKIkWGpBid"
      },
      "outputs": [],
      "source": [
        "if 'doeid' in merged_df.columns:\n",
        "    merged_df.rename(columns={'doeid': 'DOEID'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70FA-qg2pNjp"
      },
      "outputs": [],
      "source": [
        "merged_data = pd.merge(merged_df, database3, on=\"DOEID\")\n",
        "# And we merged_df with database 3 using DOEID columns. We named it merged_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWj8PuWTpRop"
      },
      "outputs": [],
      "source": [
        "print(merged_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAFu5V0EsBsm"
      },
      "source": [
        "#DETECTION OF MISSING VALUES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKWaH3u6pZ39"
      },
      "outputs": [],
      "source": [
        "merged_data.isnull().sum().sum()\n",
        "\n",
        "#This code checks if we had missing values in merged_data\n",
        "#isnull() returns a dataframe with True for NaN values and False for other values. %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEhrg7j9p8DQ"
      },
      "outputs": [],
      "source": [
        "num_duplicates = merged_data.duplicated().sum()\n",
        "# This line detects the number of duplicate rows in the merged_data dataframe.\n",
        "# sum() counts how many True values (duplicates) are in the series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnXXgwHuqVsE"
      },
      "outputs": [],
      "source": [
        "print(\"Number of duplicates in the DataFrame:\", num_duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t421rWvptVXP"
      },
      "outputs": [],
      "source": [
        "print(merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U07kYduWtVLZ"
      },
      "outputs": [],
      "source": [
        "merged_data['time'] = pd.to_datetime(merged_data['time'], format='%b')\n",
        "# This line converts the time column in the merged_data dataframe to a proper datetime format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muB0G49TtU84"
      },
      "outputs": [],
      "source": [
        "sum_by_DOEID = merged_data.groupby('DOEID').agg({'HDD65': 'sum', 'CDD65': 'sum', 'time': 'first'}).reset_index()\n",
        "#This line groups the data by the DOEID column and calculates the sum of HDD65 and CDD65 for each group.The first occurrence of time for each group is kept (using 'first')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohvHp8M7gsfF"
      },
      "outputs": [],
      "source": [
        "sum_by_DOEID['time'] = '2020'\n",
        "#This line assigns the value '2020' to all rows in the time column of sum_by_DOEID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J_P7UX2gsrZ"
      },
      "outputs": [],
      "source": [
        "sum_by_DOEID.rename(columns={'HDD65': 'HDD65_sum', 'CDD65': 'CDD65_sum'}, inplace=True)\n",
        "#This line renames the columns HDD65 and CDD65 to HDD65_sum and CDD65_sum respectively to indicate that these columns represent the sum of these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7ozlfoFgsuS"
      },
      "outputs": [],
      "source": [
        "merged_data.drop_duplicates(subset=['DOEID'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_RMaV5ggsxJ"
      },
      "outputs": [],
      "source": [
        "merged_data = merged_data.merge(sum_by_DOEID, on='DOEID', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2huGjdfdgs0F"
      },
      "outputs": [],
      "source": [
        "print(merged_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfawB2qtgs3h"
      },
      "outputs": [],
      "source": [
        "merged_data.drop(['time_x', 'HDD65', 'CDD65'], axis=1, inplace=True)\n",
        "# These are the names of the columns to be dropped from the merged_data dataframe.\n",
        "#The time_x column is likely created due to a merge operation that caused a suffix (like _x or _y) to differentiate columns with the same name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzFKXfceh175"
      },
      "outputs": [],
      "source": [
        "print(merged_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfk3sQ_0jXe2"
      },
      "source": [
        "##ANALYSIS OF MISSING VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSj8ztkIh2BQ"
      },
      "outputs": [],
      "source": [
        "merged_data.isnull().sum().to_csv('missing_values.csv')\n",
        "#merged_data.isnull() returns a dataframe where each cell is either True (if the value is missing) or False (if the value is not missing).\n",
        "#.sum() counts the True values in each column, which represents the number of missing values per column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9HJrC5Qh2Du"
      },
      "outputs": [],
      "source": [
        "columns_with_missing_values = merged_data.columns[merged_data.isnull().any()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkOg_cuKh2GE"
      },
      "outputs": [],
      "source": [
        "print(\"Columns with missing values in merged_data:\")\n",
        "print(columns_with_missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZu0jYkbh2Nq"
      },
      "outputs": [],
      "source": [
        "median_medicaldev = merged_data['medicaldev'].median()\n",
        "merged_data['medicaldev'].fillna(median_medicaldev, inplace=True)\n",
        "#calculates the median of the medicaldev column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XGBEewVh2QS"
      },
      "outputs": [],
      "source": [
        "merged_data.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLiC-00Zh2Si"
      },
      "outputs": [],
      "source": [
        "print(merged_data.medicaldev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BYZC0dwoCNO"
      },
      "source": [
        "## ERRORS AND OUTLIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGNfDOxmn1Ml"
      },
      "outputs": [],
      "source": [
        "description = merged_data.describe(include=\"all\")\n",
        "description.to_csv(\"description.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V11rqLgen1PT"
      },
      "outputs": [],
      "source": [
        "print(merged_data['totrooms'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipJL76lsn1U2"
      },
      "outputs": [],
      "source": [
        "invalid_totrooms = merged_data[(merged_data['totrooms'] < 1) | (merged_data['totrooms'] > 15)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vqIg95Ln1Z5"
      },
      "outputs": [],
      "source": [
        "merged_data= merged_data.drop(invalid_totrooms.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5RymKKWn1cp"
      },
      "outputs": [],
      "source": [
        "print(invalid_totrooms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHrjCAWin1fU"
      },
      "outputs": [],
      "source": [
        "print(merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rc-YUZ4n1h3"
      },
      "outputs": [],
      "source": [
        "possible_outliers = ['HDD65_sum', 'CDD65_sum', 'hdd30yr_pub', 'cdd30yr_pub', 'sqftest', 'totsqft_en', 'tothsqft', 'totcsqft']\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=merged_data[possible_outliers], orient='h')\n",
        "plt.title('Boxplots of selected variables')\n",
        "plt.xlabel('Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofb6BVtQn1km"
      },
      "outputs": [],
      "source": [
        "def detect_outliers_tukey(data):\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = (data < lower_bound) | (data > upper_bound)\n",
        "    return outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaZ02wLjn1o-"
      },
      "outputs": [],
      "source": [
        "columns_to_check = ['HDD65_sum', 'CDD65_sum', 'hdd30yr_pub', 'cdd30yr_pub', 'sqftest', 'totsqft_en', 'tothsqft', 'totcsqft']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ffFSryhn1sP"
      },
      "outputs": [],
      "source": [
        "for column in columns_to_check:\n",
        "    outliers = detect_outliers_tukey(merged_data[column])\n",
        "    print(f\"Number of outliers in'{column}': {outliers.sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM1CyDgDvr5o"
      },
      "outputs": [],
      "source": [
        "outliers_HDD65_sum = detect_outliers_tukey(merged_data['HDD65_sum'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sROJca8-vr8W"
      },
      "outputs": [],
      "source": [
        "outliers_hdd30yr_pub = detect_outliers_tukey(merged_data['hdd30yr_pub'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yq94CIivr-_"
      },
      "outputs": [],
      "source": [
        "common_outliers_HDD = merged_data.loc[outliers_HDD65_sum & outliers_hdd30yr_pub, 'DOEID']\n",
        "\n",
        "total_common_outliers_HDD2 = len(common_outliers_HDD)\n",
        "print(\"Number of DOEIDs in common for outliers between 'HDD65_sum' and 'hdd30yr_pub':\", total_common_outliers_HDD2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxQw_eE6vsBt"
      },
      "outputs": [],
      "source": [
        "hdd_data = merged_data['HDD65_sum']\n",
        "plt.hist(hdd_data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Histogram of HDD65_sum')\n",
        "plt.xlabel('Values of HDD65_sum')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl5qMRiWvsEe"
      },
      "outputs": [],
      "source": [
        "hdd_data_without_outliers = merged_data.loc[~outliers_HDD65_sum, 'HDD65_sum']\n",
        "plt.hist(hdd_data_without_outliers, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Histogram of HDD65_sum (without outliers)')\n",
        "plt.xlabel('Values of HDD65_sum')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wxGEmIPvsHO"
      },
      "outputs": [],
      "source": [
        "outliers_TOTSQFT_EN = detect_outliers_tukey(merged_data['totsqft_en'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIkA0c710-nD"
      },
      "outputs": [],
      "source": [
        "outliers_TOTHSQFT = detect_outliers_tukey(merged_data['tothsqft'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3acxdsRS0-pr"
      },
      "outputs": [],
      "source": [
        "outliers_TOTCSQFT = detect_outliers_tukey(merged_data['totcsqft'])\n",
        "\n",
        "outliers_communs_TOTSQFT = outliers_TOTSQFT_EN & outliers_TOTHSQFT & outliers_TOTCSQFT\n",
        "\n",
        "shared_outliers_TOTSQFT = np.sum(outliers_communs_TOTSQFT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WgGqEkv0-sU"
      },
      "outputs": [],
      "source": [
        "print(\"Number of outliers in commun between TOTSQFT_EN, TOTHSQFT and TOTCSQFT:\", shared_outliers_TOTSQFT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOzZ8rY03HlV"
      },
      "source": [
        "##REMOVE OUTLIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a-P_LHB0-xr"
      },
      "outputs": [],
      "source": [
        "variables_to_process = ['HDD65_sum', 'CDD65_sum', 'hdd30yr_pub', 'cdd30yr_pub', 'sqftest', 'totsqft_en', 'tothsqft', 'totcsqft']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-vnCBoQ0-0Q"
      },
      "outputs": [],
      "source": [
        "for variable in variables_to_process:\n",
        "    outliers = detect_outliers_tukey(merged_data[variable])\n",
        "    merged_data = merged_data[~outliers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBBqFl670-2n"
      },
      "outputs": [],
      "source": [
        "print(\"Size of DataFrame after removing outliers:\", merged_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD6rmfYC0-71"
      },
      "outputs": [],
      "source": [
        "shape_dataset = merged_data.shape\n",
        "\n",
        "print(\"Shape of dataset :\", shape_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wysdj-tf5mQb"
      },
      "source": [
        "##CHECKING SINGLE VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJvuP-vE0_A4"
      },
      "outputs": [],
      "source": [
        "for column in merged_data.columns:\n",
        "\n",
        "    print(f\"Unique values in the column '{column}':\")\n",
        "    print(merged_data[column].unique())\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9N-KL9U6eSU"
      },
      "source": [
        "##CONVERSION OF VARIABLES IN \"INT64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IPlXqQP0_F8"
      },
      "outputs": [],
      "source": [
        "categorical_cols = []\n",
        "\n",
        "new_data = merged_data.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCZHkxjdxIA-"
      },
      "outputs": [],
      "source": [
        "print(new_data.columns.tolist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRJ6UhyyEgp8"
      },
      "outputs": [],
      "source": [
        "copied_data = new_data.iloc[:, 9:293]\n",
        "\n",
        "print(copied_data.head(290))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8me_aRv0_In"
      },
      "outputs": [],
      "source": [
        "new_categorical_cols = [ 'typehuq', 'cellar', 'crawl', 'concrete', 'baseoth', 'basefin', 'attic', 'atticfin', 'stories', 'prkgplc1', 'sizeofgarage', 'kownrent',\n",
        "                        'yearmaderange', 'bedrooms', 'ncombath', 'nhafbath', 'othrooms', 'totrooms',\n",
        "                        'studio', 'walltype', 'rooftype', 'highceil', 'door1sum', 'windows', 'typeglass',\n",
        "                        'origwin', 'winframe', 'treeshad', 'adqinsul', 'drafty', 'ugashere', 'swimpool',\n",
        "                        'monpool', 'poolpump', 'fuelpool', 'recbath', 'montub', 'fueltub', 'numfrig',\n",
        "                        'sizrfri1', 'typerfr1', 'agerfri1', 'ice', 'sizrfri2', 'typerfr2', 'agerfri2',\n",
        "                        'locrfri2', 'winechill', 'numfreez', 'uprtfrzr', 'sizfreez', 'freezer', 'agefrzr',\n",
        "                        'range', 'cooktop', 'oven', 'rangefuel', 'rangeindt', 'rcookuse', 'rovenuse',\n",
        "                        'cooktopfuel', 'cooktopindt', 'cooktopuse', 'ovenfuel', 'ovenuse', 'micro',\n",
        "                        'amtmicro', 'outgrillfuel', 'outgrill', 'nummeal', 'usecoffee', 'toast',\n",
        "                        'toastovn', 'crockpot', 'prsscook', 'ricecook', 'blender', 'appother', 'elfood',\n",
        "                        'lpcook', 'ugcook', 'dishwash', 'dwashuse', 'dwcycle', 'agedw', 'cwasher',\n",
        "                        'topfront', 'washload', 'washtemp', 'agecwash', 'dryer', 'dryrfuel', 'dryruse',\n",
        "                        'agecdryer', 'tvcolor', 'tvsize1', 'tvtype1', 'tvuse1', 'tvonwd1', 'tvonwe1',\n",
        "                        'tvsize2', 'tvtype2', 'tvuse2', 'tvonwd2', 'tvonwe2', 'tvsize3', 'tvtype3',\n",
        "                        'tvuse3', 'tvonwd3', 'tvonwe3', 'cablesat', 'combodvr', 'sepdvr', 'intstream',\n",
        "                        'playsta', 'dvd', 'vcr', 'tvaudiosys', 'desktop', 'numlaptop', 'numtablet',\n",
        "                        'elperiph', 'numsmphone', 'cellphone', 'tellwork', 'telldays', 'tldesktop',\n",
        "                        'tllaptop', 'tltablet', 'tlmonitor', 'tlother', 'onlneduc', 'internet', 'intypecell',\n",
        "                        'intypebroad', 'intypeoth', 'smartspk', 'sslight', 'sstemp', 'sssecure', 'sstv',\n",
        "                        'ssother', 'heathome', 'dntheat', 'heatapt', 'equipm', 'fuelheat', 'equipage', 'geohp',\n",
        "                        'equipauxtype', 'equipaux', 'fuelaux', 'useequipaux', 'numportel', 'numfireplc', 'numdlhp',\n",
        "                        'baseheat', 'attcheat', 'gargheat', 'humidtype', 'numporthum', 'usehumid', 'elwarm', 'ugwarm',\n",
        "                        'lpwarm', 'fowarm', 'wdwarm', 'aircond', 'coolapt', 'acequipm_pub', 'acequipage',\n",
        "                        'acequipauxtype_pub', 'numdlhpac', 'numwwac', 'numportac', 'basecool', 'attccool', 'gargcool',\n",
        "                        'numcfan', 'numfloorfan', 'usecfan', 'housefan', 'atticfan', 'dehumtype', 'numportdehum',\n",
        "                        'usedehum', 'elcool', 'typetherm', 'heatcntl', 'temphome', 'tempgone', 'tempnite', 'coolcntl',\n",
        "                        'temphomeac', 'tempgoneac', 'tempniteac', 'h2oapt', 'h2omain', 'wheatsiz', 'wheatbkt', 'wheatage',\n",
        "                        'fuelh2o', 'morethan1h2o', 'fuelh2o2', 'elwater', 'fowater', 'lpwater', 'solwater', 'wdwater',\n",
        "                        'ugwater', 'lgtin1to4', 'lgtin4to8', 'lgtinmore8', 'lgtinled', 'lgtincfl', 'lgtincan', 'lgtoutany',\n",
        "                        'lgtoutnite', 'lgtoutled', 'lgtoutcfl', 'lgtoutcan', 'elpay', 'ngpay', 'lpgpay', 'fopay', 'smartmeter',\n",
        "                        'intdataacc', 'medicaldev', 'powerout', 'whypowerout', 'backup', 'solar', 'elother', 'ugoth', 'lpother',\n",
        "                        'foother', 'useel', 'useng', 'uselp', 'usefo', 'usesolar', 'usewood', 'allelec', 'sqftest', 'sqftrange',\n",
        "                        'sqftincb', 'sqftinca', 'sqftincg', 'totsqft_en', 'tothsqft', 'totcsqft', 'dbt1', 'dbt99', 'gwt',\n",
        "                        'woodtype', 'outlet', 'elecveh', 'evchrghome', 'evchrgapt', 'evchrgwks', 'evchrgbus', 'evchrgmuni',\n",
        "                        'evchrgdlr', 'evchrghwy', 'evchrgoth', 'evhomeamt', 'evchrgtype', 'REGIONC', 'DIVISION', 'STATE_FIPS',\n",
        "                        'state_postal', 'state_name', 'HHSEX', 'HHAGE', 'EMPLOYHH', 'EDUCATION', 'SDESCENT', 'HOUSEHOLDER_RACE',\n",
        "                        'NHSLDMEM', 'NUMCHILD', 'NUMADULT1', 'NUMADULT2', 'ATHOME', 'MONEYPY',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0F2v46m_d8t"
      },
      "outputs": [],
      "source": [
        "print(new_data.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmXgUSVVyoLE"
      },
      "outputs": [],
      "source": [
        "print(copied_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVavB8Ns0_Lc"
      },
      "outputs": [],
      "source": [
        "categorical_cols.extend(new_categorical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm_digifvFw-"
      },
      "outputs": [],
      "source": [
        "# Ensure all columns in 'categorical_cols' are present in 'new_data' and are unique.\n",
        "categorical_cols = list(set(categorical_cols) & set(copied_data.columns))\n",
        "\n",
        "# Convert the selected columns to categorical\n",
        "copied_data[categorical_cols] = copied_data[categorical_cols].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcCv0b180_Rn"
      },
      "outputs": [],
      "source": [
        "print(copied_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYwZjra92siD"
      },
      "outputs": [],
      "source": [
        "copied_data = new_data.iloc[:, 9:293]  # Colonnes 10 à 293\n",
        "\n",
        "# we identify the missing columns\n",
        "remaining_columns = [col for col in new_data.columns if col not in copied_data.columns]\n",
        "\n",
        "copied_data = pd.concat([copied_data, new_data[remaining_columns]], axis=1)\n",
        "\n",
        "print(copied_data.shape)\n",
        "print(copied_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsEX_CMFw9tj"
      },
      "outputs": [],
      "source": [
        "print(copied_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozv2TNzg0_cI"
      },
      "outputs": [],
      "source": [
        "copied_data.to_csv('database_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_VGzq95UUH"
      },
      "source": [
        "##LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMLRYFn_bctf"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.tree._export import plot_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-CkAP7AJ2uc"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade xgboost scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_W3c_umLcwM"
      },
      "outputs": [],
      "source": [
        "print(\"Version of XGBoost :\", xgboost.__version__)\n",
        "print(\"Version of scikit-learn :\", sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpGSppPs6ZYF"
      },
      "outputs": [],
      "source": [
        "# We had to make sure  that 'copied_data' contains our data\n",
        "# Checking of  available columns\n",
        "print(copied_data.columns)\n",
        "\n",
        "# We created the 'consumption_level' target based on the median of HDD65_sum\n",
        "copied_data['consumption_level'] = np.where(\n",
        "    copied_data['HDD65_sum'] <= copied_data['HDD65_sum'].median(), 0, 1\n",
        ")\n",
        "\n",
        "# We separate explanatory variables (X) and target (y)\n",
        "X = copied_data[['HDD65_sum', 'CDD65_sum', 'totsqft_en', 'NUMADULT1', 'tvsize1', 'dishwash']]\n",
        "y = copied_data['consumption_level']\n",
        "\n",
        "#  We managed missing values\n",
        "X = X.fillna(X.median())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VafqP1Xs8N3b"
      },
      "outputs": [],
      "source": [
        "# 1. Creation of  the target column (High or Low) based on a reference column\n",
        "mean_consumption = copied_data['HDD65_sum'].mean()\n",
        "copied_data['consumption_level'] = copied_data['HDD65_sum'].apply(lambda x: 'Low' if x < mean_consumption else 'High')\n",
        "copied_data['consumption_level'] = copied_data['consumption_level'].map({'Low': 0, 'High': 1})\n",
        "\n",
        "# 2. Data preparation (we have included  CDD65 and other relevant variables)\n",
        "X = copied_data[['HDD65_sum', 'CDD65_sum', 'totsqft_en', 'NUMADULT1', 'tvsize1', 'dishwash']]  # Inclure CDD65_sum et HDD65_sum\n",
        "y = copied_data['consumption_level']\n",
        "\n",
        "# 3.data normalization because Machine learning models are sensitive to the range of data values.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 4. Division of data into training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# 5. Model training\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "# 7. Evaluation\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 8. Visualization of the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8EnhX3apCK7"
      },
      "outputs": [],
      "source": [
        "print(copied_data[\"consumption_level\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vziyaYNxplUN"
      },
      "outputs": [],
      "source": [
        "# Histogram for consumption by TV size\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='tvsize1', hue='consumption_level', data=copied_data, palette=\"coolwarm\")\n",
        "plt.title('Distribution de TV Size par Consumption Level', fontsize=14)\n",
        "plt.xlabel('Taille de la TV (tvsize)', fontsize=12)\n",
        "plt.ylabel('Count(tvsize1)', fontsize=12)\n",
        "plt.legend(title='Consumption Level', labels=['Low (0)', 'High (1)'], fontsize=10)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy0Z_fiSrIHd"
      },
      "outputs": [],
      "source": [
        "#  Histogram for consumption for diswash\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='dishwash', hue='consumption_level', data=copied_data, palette=\"coolwarm\")\n",
        "plt.title('Consumption Level', fontsize=14)\n",
        "plt.xlabel('Diswash', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.legend(title='Consumption Level', labels=['Low (0)', 'High (1)'], fontsize=10)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chXlrootskFD"
      },
      "outputs": [],
      "source": [
        "correlation = copied_data[\"HDD65_sum\"].corr(copied_data[\"CDD65_sum\"])\n",
        "# Correlation to measure the strength and direction of the linear relationship between HDD65_sum and CDD65_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVmVwvuasx0x"
      },
      "outputs": [],
      "source": [
        "print(correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdSsRzjHs3sm"
      },
      "outputs": [],
      "source": [
        "correlation = copied_data[\"CDD65_sum\"].corr(copied_data[\"consumption_level\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ygfy5S58N5v"
      },
      "outputs": [],
      "source": [
        "X = copied_data[['HDD65_sum', 'CDD65_sum']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p83BNbdU8N8a"
      },
      "outputs": [],
      "source": [
        "target = pd.DataFrame ({\n",
        "    'other_column': [1, 0],\n",
        "    'consumption_level': ['Low', 'High']})\n",
        "# This code creates a DataFrame called target that maps numerical values (1 and 0) to their corresponding text labels (Low and High) to facilitate the conversion between these formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrQrqvY28N_J"
      },
      "outputs": [],
      "source": [
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq2zjg-i8OBg"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmIea-luIflZ"
      },
      "outputs": [],
      "source": [
        "print(copied_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DZUxdprIfqL"
      },
      "outputs": [],
      "source": [
        "copied_data['Household_id'] = range(1, len(copied_data) + 1)\n",
        "print(copied_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoKrNJygFEkV"
      },
      "outputs": [],
      "source": [
        "copied_data = copied_data.drop(columns=['Household_id'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kLyK1taIfsG"
      },
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'Household_id': copied_data[\"HHID\"][:len(y_pred)],\n",
        "    'predicted_consumption': y_pred\n",
        "})\n",
        "#creation of  a DataFrame named predictions_df, which is designed to store predicted consumption data for households"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uug6RQtTIfwg"
      },
      "outputs": [],
      "source": [
        "print(predictions_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJbSzeyI9mVa"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = copied_data.drop(columns=['DOEID', 'consumption_level'])\n",
        "y = copied_data['consumption_level']\n",
        "#This code splits the dataset copied_data into features (X) by dropping the columns DOEID and consumption_level, and the target variable (y), which is set as the consumption_level column ensuring clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ATV9teh9mYF"
      },
      "outputs": [],
      "source": [
        "X = pd.get_dummies(X, drop_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgoy0VFq9maw"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xwmxLei9mdQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.8, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m3Yiopc9mgS"
      },
      "outputs": [],
      "source": [
        "\n",
        "logistic_model = LogisticRegression(max_iter=1000)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "#Initialization  and training of a logistic regression model.\n",
        "#The model learns the relationship between the features and target,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAX4CbSh9mjm"
      },
      "outputs": [],
      "source": [
        "y_pred = logistic_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBq-svZ19m40"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wftJDhK-9m7b"
      },
      "outputs": [],
      "source": [
        "copied_data['predicted_consumption_level'] = logistic_model.predict(X_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7yVWp-59m-G"
      },
      "outputs": [],
      "source": [
        "data_with_predictions = copied_data[['DOEID', 'predicted_consumption_level']]\n",
        "data_with_predictions.to_csv('predictions_par_menage.csv', index=False)\n",
        "# Extracts the DOEID and predicted_consumption_level columns from the dataset and saves them to a CSV file named predictions_par_menage.csv.\n",
        "# Excluding row indices focusing on relevant columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmX9Yw0T9nBe"
      },
      "outputs": [],
      "source": [
        "print(data_with_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDy_xLkwAnl4"
      },
      "outputs": [],
      "source": [
        "data_with_predictions_sorted = data_with_predictions.sort_values(by='DOEID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea06WismAnok"
      },
      "outputs": [],
      "source": [
        "print(data_with_predictions_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfNLa02hAnt3"
      },
      "outputs": [],
      "source": [
        "print(data_with_predictions.head(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtG3zi-OcCA"
      },
      "source": [
        "##LOGISTIC REGRESSION WITH PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4NmmE8WwN-1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfp1A61AwOCD"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "lr = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
        "\n",
        "model1 = Pipeline([('standardize', scaler),\n",
        "                    ('log_reg', lr)])\n",
        "model1\n",
        "#model1 is a pipeline that Standardizes the features,\n",
        "#Trains a logistic regression model using the standardized data.\n",
        "#This structure ensures a clean, efficient, and error-free workflow for preprocessing and modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrqI9pvSwOFZ"
      },
      "outputs": [],
      "source": [
        "model1.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYbEgUW5xQHE"
      },
      "outputs": [],
      "source": [
        "model1.get_params\n",
        "#returns the pipeline parameters, useful for inspection or modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HodzJiIIxQUj"
      },
      "outputs": [],
      "source": [
        "# Now predict\n",
        "y_train_hat = model1.predict(X_train)\n",
        "y_train_hat[:100]\n",
        "#This code uses the model to predict labels on `X_train` and displays the first 100 predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFZq087YxQXj"
      },
      "outputs": [],
      "source": [
        "y_train_hat_probs = model1.predict_proba(X_train)[:,1] # probabilities of being in class 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG6k94M2xQai"
      },
      "outputs": [],
      "source": [
        "temp= pd.DataFrame({'y_train_hat': y_train_hat, 'y_train_hat_probs': y_train_hat_probs})\n",
        "temp.head(100)\n",
        "#creates a DataFrame temp with two columns: y_train_hat (predicted labels) and y_train_hat_probs (probabilities for the positive class),\n",
        "#and displays the first 100 rows for inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhXqFqRfxQdk"
      },
      "outputs": [],
      "source": [
        "temp['y_train_hat_probs'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGNdQlNsxQiQ"
      },
      "outputs": [],
      "source": [
        "train_accuracy = accuracy_score(y_train, y_train_hat)*100\n",
        "train_auc_roc = roc_auc_score(y_train, y_train_hat_probs)*100\n",
        "\n",
        "print('Confusion matrix:\\n', confusion_matrix(y_train, y_train_hat))\n",
        "print('Training AUC: %.4f %%' % train_auc_roc)\n",
        "print('Training accuracy: %.4f %%' % train_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnh0HpYgxQnn"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_test_hat, digits=6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-RVJRnKOJd9"
      },
      "source": [
        "##BAGGINGCLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rM_D6g12LRC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ4bXvWAz2N5"
      },
      "outputs": [],
      "source": [
        "# Preparation of the data\n",
        "X = copied_data[['HDD65_sum', 'CDD65_sum', 'totsqft_en', 'NUMADULT1', 'tvsize1', 'dishwash']]  # Features pertinentes\n",
        "y = copied_data['consumption_level']  # target variable\n",
        "\n",
        "# Division into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Creation of the Bagging model\n",
        "# Use estimator instead of base_estimator if your scikit-learn version is older.\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Changed base_estimator to estimator\n",
        "    n_estimators=50,  # Number of estimators\n",
        "    max_samples=0.8,  # Size of sub-sample\n",
        "    random_state=42\n",
        ")\n",
        "#This model creates an ensemble of 50 decision trees, each trained on 80% of the dataset,\n",
        "# and aggregates their predictions (e.g., majority voting) to improve accuracy\n",
        "#and reduce overfitting compared to a single Decision Tree.\n",
        "\n",
        "# model training\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# performance evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
        "plt.title(\"Matrice de Confusion - Bagging\")\n",
        "plt.xlabel(\"Prédictions\")\n",
        "plt.ylabel(\"Valeurs Réelles\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgrXSt7AN-PG"
      },
      "source": [
        "##XGBOOST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TKpjLjeAny8"
      },
      "outputs": [],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jMGxiTkGELh"
      },
      "outputs": [],
      "source": [
        "print(data_with_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HEyJDG5E_u0"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = copied_data[['HDD65_sum', 'CDD65_sum', 'totsqft_en', 'NUMADULT1', 'tvsize1', 'dishwash']]\n",
        "y = copied_data['consumption_level']\n",
        "#selects specific columns from copied_data to create the features (X) and the target variable (y)\n",
        "\n",
        "\n",
        "# Separate the features and the target\n",
        "#X = data_with_predictions.drop(['DOEID', 'predicted_consumption_level'], axis=1)  # Features\n",
        "X = copied_data[['HDD65_sum', 'CDD65_sum', 'totsqft_en', 'NUMADULT1', 'tvsize1', 'dishwash']]  # Use of the original features\n",
        "y = copied_data['consumption_level']  # Target\n",
        "\n",
        "# Split data  in training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Initialisation of XGBOOST model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = xgb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5KQ9xHwE_x6"
      },
      "outputs": [],
      "source": [
        "# Calculates the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNiC48b1E_0z"
      },
      "outputs": [],
      "source": [
        "# We add the predictions to the original DataFrame\n",
        "data_with_predictions['xgb_predicted_consumption_level'] = xgb_model.predict(X)\n",
        "\n",
        "# Display the results\n",
        "print(data_with_predictions[['DOEID', 'xgb_predicted_consumption_level']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yos2Qjw2E_3o"
      },
      "outputs": [],
      "source": [
        "data_with_predictions.to_csv('predictions_with_xgboost.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ycrxQrlE_6V"
      },
      "outputs": [],
      "source": [
        "# Save  the  predictions in a CSV file\n",
        "data_with_predictions.to_csv('predictions_with_xgboost.csv', index=False)\n",
        "\n",
        "# Uploading file to Jupyter Notebook environment\n",
        "files.download('predictions_with_xgboost.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM-UWL-5E__j"
      },
      "outputs": [],
      "source": [
        "# Replace the values in the 'predicted_consumption_level’column\n",
        "data_with_predictions['predicted_consumption_level'] = data_with_predictions['predicted_consumption_level'].replace({1: 'high', 0: 'low'})\n",
        "\n",
        "# Then we check the  modifications\n",
        "print(data_with_predictions.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdYlukAEJrOb"
      },
      "outputs": [],
      "source": [
        "# Replace tha values in the  'predicted_consumption_level' column\n",
        "data_with_predictions['xgb_predicted_consumption_level'] = data_with_predictions['xgb_predicted_consumption_level'].replace({1: 'high', 0: 'low'})\n",
        "\n",
        "# Then check  modifications again\n",
        "print(data_with_predictions.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngk3BIL7J32B"
      },
      "outputs": [],
      "source": [
        "data_with_predictions.to_excel('predictions_with_xgboost.xlsx', index=False)\n",
        "\n",
        "files.download('predictions_with_xgboost.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84Qj0Et1J41M"
      },
      "outputs": [],
      "source": [
        "copied_data['HDD65_sum'].hist(bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution de HDD65_sum')\n",
        "plt.xlabel('HDD65_sum')\n",
        "plt.ylabel('Fréquence')\n",
        "plt.show()\n",
        "# We enerates a histogram to visualize the distribution of values in the HDD65_sum column from the copied_data DataFrame\n",
        "#The histogram helps to understand the distribution of heating degree days (HDD65_sum) in the dataset,\n",
        "#revealing whether the data is symmetrical, skewed, or multimodal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPlMJ59zMTtB"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x='predicted_consumption_level', y='HDD65_sum', data=copied_data, palette='Blues')\n",
        "plt.title('Distribution de HDD65_sum par niveau de consommation')\n",
        "plt.xlabel('Niveau de consommation')\n",
        "plt.ylabel('HDD65_sum')\n",
        "plt.xticks(ticks=[0, 1], labels=['Low', 'High'])\n",
        "plt.show()\n",
        "#This code generates a boxplot (whisker box) to visualize the distribution of the values of HDD65_sum\n",
        "#according to the predicted consumption levels (predicted_consumption_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMFB18v8MUJo"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of the Low and High predictions\n",
        "prediction_counts = copied_data['predicted_consumption_level'].value_counts()\n",
        "\n",
        "#Histogram\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(prediction_counts.index.map({0: 'Low', 1: 'High'}), prediction_counts.values, color=['skyblue', 'salmon'], edgecolor='black')\n",
        "plt.title(\"Predictions : Low vs High\", fontsize=14)\n",
        "plt.xlabel(\"Consumption level predicted\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBK9GcJAIi9f"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(logistic_model, X_scaled, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validation scores :\", scores)\n",
        "print(\"average score :\", scores.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4Z0qP4bIjAZ"
      },
      "outputs": [],
      "source": [
        "print(copied_data.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOsRUNasIjJe"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x='consumption_level', y='totsqft_en', data=copied_data, palette='coolwarm')\n",
        "plt.title(\"The range of the household according to the level of consumption\")\n",
        "plt.xlabel(\"consumption level (0=Low, 1=High)\")\n",
        "plt.ylabel(\"House size (in square feet)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGodx3io-Cnb"
      },
      "outputs": [],
      "source": [
        "# Add a categorical column to identify areas\n",
        "#identifies and visualizes climate zones based on HDD65_sum (heating degree days) and CDD65_sum (cooling degree days):\n",
        "def classify_zone(row):\n",
        "    if row['HDD65_sum'] > 6000 and row['CDD65_sum'] < 1000:\n",
        "        return 'Cold Zone'\n",
        "    elif row['HDD65_sum'] < 1000 and row['CDD65_sum'] > 2000:\n",
        "        return 'Hot Zone'\n",
        "    else:\n",
        "        return 'Mixed Zone'\n",
        "\n",
        "copied_data['zone'] = copied_data.apply(classify_zone, axis=1)\n",
        "\n",
        "# Check of the  distributions\n",
        "print(copied_data['zone'].value_counts())\n",
        "\n",
        "# Visualisation of areas\n",
        "sns.scatterplot(x='HDD65_sum', y='CDD65_sum', hue='zone', data=copied_data, palette='coolwarm')\n",
        "plt.title(\"Zones identifiées (Cold, Hot, Mixed)\")\n",
        "plt.xlabel(\"HDD65_sum (jours de chauffage)\")\n",
        "plt.ylabel(\"CDD65_sum (jours de climatisation)\")\n",
        "plt.legend(title=\"Zone\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcYOTmn1-C2t"
      },
      "outputs": [],
      "source": [
        "#evaluates how well a model performs on labeled data\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, pos_label='high')\n",
        "    recall = recall_score(y_true, y_pred, pos_label='high')\n",
        "    f1 = f1_score(y_true, y_pred, pos_label='high')\n",
        "\n",
        "    print(f\"Évaluation du modèle : {model_name}\")\n",
        "    print(f\"Accuracy : {accuracy:.2f}\")\n",
        "    print(f\"Precision : {precision:.2f}\")\n",
        "    print(f\"Recall : {recall:.2f}\")\n",
        "    print(f\"F1-Score : {f1:.2f}\")\n",
        "    print(\"Matrice de confusion :\")\n",
        "    print(confusion_matrix(y_true, y_pred, labels=['low', 'high']))\n",
        "    print(\"\\n\")\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Comparison of the two models\n",
        "rf_metrics = evaluate_model(data_with_predictions['predicted_consumption_level'],\n",
        "                            data_with_predictions['xgb_predicted_consumption_level'], \"Random Forest\")\n",
        "xgb_metrics = evaluate_model(data_with_predictions['predicted_consumption_level'],\n",
        "                             data_with_predictions['xgb_predicted_consumption_level'], \"XGBoost\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}